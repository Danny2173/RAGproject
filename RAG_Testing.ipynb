{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOcn+OaGIXpn3Cg8hwkBhQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danny2173/RAGproject/blob/main/RAG_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install gradio -q\n",
        "\n",
        "# Imports\n",
        "import os, json, re, pickle, itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import faiss\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from transformers import (\n",
        "    DPRQuestionEncoder,\n",
        "    DPRQuestionEncoderTokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DPRContextEncoder,\n",
        "    DPRContextEncoderTokenizer,\n",
        "    LogitsProcessor,\n",
        "    LogitsProcessorList\n",
        ")\n",
        "from peft import PeftModel\n",
        "import gradio as gr\n",
        "\n",
        "!git clone -q https://github.com/Danny2173/RAGproject.git /content/RAGproject\n",
        "%cd /content/RAGproject"
      ],
      "metadata": {
        "id": "7cCRpW2PxA2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Load tokenizer and LoRA model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")\n",
        "model = PeftModel.from_pretrained(base_model, \"/content/RAGproject/LoRA/t5-lora-final\")\n",
        "model = model.to(device)\n",
        "\n",
        "with open(\"/content/RAGproject/FAISS/corpus_texts.pkl\", \"rb\") as f:\n",
        "    corpus_texts = pickle.load(f)\n",
        "\n",
        "with open(\"/content/RAGproject/FAISS/normalized_for_index.pkl\", \"rb\") as f:\n",
        "    normalized_for_index = pickle.load(f)\n",
        "\n",
        "with open(\"/content/RAGproject/FAISS/corpus.pkl\", \"rb\") as f:\n",
        "    corpus = pickle.load(f)\n",
        "\n",
        "index = faiss.read_index(\"/content/RAGproject/FAISS/faiss_index.index\")\n",
        "\n",
        "\n",
        "def count_heading_term_matches(doc_text, matched_main_terms):\n",
        "    # Normalize text and main terms\n",
        "    doc_text = doc_text.lower()\n",
        "    matched_main_terms = set(matched_main_terms)\n",
        "\n",
        "    # Extracting titles and section headings\n",
        "    title_match = re.match(r\"^(.*?)\\n\", doc_text)\n",
        "    title = title_match.group(1) if title_match else \"\"\n",
        "\n",
        "    section_matches = re.findall(r\"^section:\\s*(.*?)$\", doc_text, flags=re.MULTILINE)\n",
        "    subsection_matches = re.findall(r\"^subsection:\\s*(.*?)$\", doc_text, flags=re.MULTILINE)\n",
        "    subsubsection_matches = re.findall(r\"^subsubsection:\\s*(.*?)$\", doc_text, flags=re.MULTILINE)\n",
        "\n",
        "    # Combining titles with section headings\n",
        "    heading_text = \" \".join([title] + section_matches + subsection_matches + subsubsection_matches)\n",
        "\n",
        "    # Tokenize heading text for comparison\n",
        "    heading_tokens = set(re.findall(r'\\b\\w[\\w\\s\\-]*\\w\\b', heading_text))\n",
        "\n",
        "    # Count matches\n",
        "    match_count = sum(1 for term in matched_main_terms if term in heading_text)\n",
        "\n",
        "    return match_count\n",
        "\n",
        "# Importing conversion functions\n",
        "\n",
        "with open(\"/content/RAGproject/Normalization/filtered_term_to_CUI.pkl\", \"rb\") as f:\n",
        "    term_to_CUI = pickle.load(f)\n",
        "\n",
        "with open(\"/content/RAGproject/Normalization/filtered_cui_to_main_term.pkl\", \"rb\") as f:\n",
        "    cui_to_main_term = pickle.load(f)\n",
        "\n",
        "# Creating ngrams and tracking indices\n",
        "def ngram_tokenize_tokens(tokens, max_len=5):\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens)):\n",
        "        for j in range(i + 1, min(i + 1 + max_len, len(tokens) + 1)):\n",
        "            span = tokens[i:j]\n",
        "            ngram = ' '.join(span)\n",
        "            ngrams.append((ngram, i, j))\n",
        "    return ngrams\n",
        "\n",
        "# Normalizing medical terms using main condition name\n",
        "def cui_normalization(sentence, max_ngram_len=5):\n",
        "    tokens = re.findall(r'\\w+|\\W+', sentence)\n",
        "\n",
        "    # Filtering out words\n",
        "    words = [w.lower() for w in tokens if re.match(r'\\w+', w)]\n",
        "\n",
        "    # Call tokenization function to return ngrams tuples\n",
        "    ngrams = ngram_tokenize_tokens(words, max_ngram_len)\n",
        "    replacements = []\n",
        "\n",
        "    # Tracking matched CUIs\n",
        "    matched_cuis = set()\n",
        "\n",
        "    # Searching for terms in dictionary\n",
        "    for ngram, start, end in ngrams:\n",
        "        if ngram in term_to_CUI:\n",
        "            cui = term_to_CUI[ngram]\n",
        "            if cui in cui_to_main_term:\n",
        "                replacements.append((start, end, cui_to_main_term[cui]))\n",
        "                matched_cuis.add(cui)\n",
        "\n",
        "    # Sorting by length then index (ensure longer terms first)\n",
        "    replacements.sort(key=lambda x: (x[0], -(x[1] - x[0])))\n",
        "    used = set()\n",
        "    final = []\n",
        "    # Ensure no overlap (check already used indices)\n",
        "    for start, end, main_term in replacements:\n",
        "        if not any(i in used for i in range(start, end)):\n",
        "            final.append((start, end, main_term))\n",
        "            used.update(range(start, end))\n",
        "\n",
        "    # Reconstruct the sentence\n",
        "    word_idx = 0\n",
        "    output = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        # If the token is a word\n",
        "        if re.match(r'\\w+', tokens[i]):\n",
        "            # Checking if index appears in final\n",
        "            match = next((f for f in final if f[0] == word_idx), None)\n",
        "            if match:\n",
        "                output.append(match[2]) # append main term\n",
        "                skip = match[1] - match[0]\n",
        "                while skip > 0 and i < len(tokens):\n",
        "                    if re.match(r'\\w+', tokens[i]):\n",
        "                        skip -= 1\n",
        "                    i += 1\n",
        "                # Update word-level index\n",
        "                word_idx += (match[1] - match[0])\n",
        "                continue\n",
        "            word_idx += 1\n",
        "        output.append(tokens[i])\n",
        "        i += 1\n",
        "\n",
        "    normalized_text = ''.join(output)\n",
        "    matched_main_terms = [term for _, _, term in final]\n",
        "    return normalized_text, matched_main_terms, list(matched_cuis)\n",
        "\n",
        "# Build TF-IDF vectorizer for exact matching\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(normalized_for_index)\n",
        "tfidf_matrix = normalize(tfidf_matrix)\n",
        "\n",
        "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
        "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")"
      ],
      "metadata": {
        "id": "y-McT5jwzBDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_pipeline_lora_top1title_concat(\n",
        "    query,\n",
        "    top_k=600,\n",
        "    bonus_weight=0.1,\n",
        "    max_answer_tokens=256,\n",
        "    confidence_threshold=0.63,\n",
        "    combine_k=4,\n",
        "    source_info=False\n",
        "):\n",
        "\n",
        "    # 1. Normalize query\n",
        "    normalized_query, matched_main_terms, matched_cuis = cui_normalization(query)\n",
        "    # 1.1 If no medical condition terms mentioned then return error\n",
        "    if not matched_main_terms:\n",
        "        return \"!Please rephrase the query using a specific medical condition!\", [], [], []\n",
        "\n",
        "    # 2. Title match indices\n",
        "    heading_matched_indices = [\n",
        "        i for i, doc_text in enumerate(normalized_for_index)\n",
        "        if any(term in doc_text.split('.')[0].lower() for term in matched_main_terms)\n",
        "    ]\n",
        "\n",
        "    # 3. FAISS search\n",
        "    inputs = q_tokenizer(normalized_query, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        q_emb = q_encoder(**inputs).pooler_output.cpu().numpy()\n",
        "        q_emb /= np.linalg.norm(q_emb, axis=1, keepdims=True)\n",
        "\n",
        "    # 3.1 Scores and indices for the top 600 documents\n",
        "    scores, indices = index.search(q_emb, top_k)\n",
        "    scores, indices = scores[0], indices[0]\n",
        "    index_scores = {idx: score for idx, score in zip(indices, scores)}\n",
        "\n",
        "\n",
        "    # 4. TF-IDF similarity score\n",
        "    tfidf_query_vec = tfidf_vectorizer.transform([normalized_query])\n",
        "    tfidf_query_vec = normalize(tfidf_query_vec)\n",
        "    tfidf_scores = (tfidf_matrix @ tfidf_query_vec.T).toarray().ravel()\n",
        "\n",
        "    # 5. Combine indices from FAISS and indices from Title-matched docs\n",
        "    all_indices = set(index_scores.keys()) | set(heading_matched_indices)\n",
        "    for idx in all_indices:\n",
        "        if idx not in index_scores:\n",
        "            index_scores[idx] = 0.1\n",
        "\n",
        "    # 6. Combine DPR and TF-IDF scores\n",
        "    boosted_index_scores = {}\n",
        "    for idx in all_indices:\n",
        "        heading_match_count = count_heading_term_matches(normalized_for_index[idx], matched_main_terms)\n",
        "        dense_score  = float(index_scores.get(idx, 0.1))\n",
        "        sparse_score = float(tfidf_scores[idx])\n",
        "        boosted_index_scores[idx] = 0.9 * dense_score + 0.1 * sparse_score + bonus_weight * heading_match_count\n",
        "\n",
        "    # 7. Sorting scores and storing scores/indices for debugging\n",
        "    sorted_items   = sorted(boosted_index_scores.items(), key=lambda x: -x[1])\n",
        "    ranked_indices = [int(i) for i, _ in sorted_items]\n",
        "    sorted_scores  = [float(s) for _, s in sorted_items]\n",
        "\n",
        "    # 8. Retrieving top-4 documents\n",
        "    used_k = min(int(combine_k), len(ranked_indices))\n",
        "    retrieved_texts = [corpus_texts[idx] for idx in ranked_indices[:used_k]]\n",
        "    boosted_used    = sorted_scores[:used_k]\n",
        "\n",
        "    # 9. Threshold-based confidence check\n",
        "    if all(s < confidence_threshold for s in boosted_used):\n",
        "        return \"!I do not have enough confidence to answer your question. Please try rephrasing it.\", retrieved_texts, boosted_used, []\n",
        "\n",
        "    # 10. Combining context and query for input\n",
        "    combined_context = \"\\n\\n\".join(txt.strip() for txt in retrieved_texts)\n",
        "    t5_input = f\"question: {query}\\ncontext: {combined_context}\"\n",
        "\n",
        "    # 11. Tokenize and track truncation\n",
        "    enc = tokenizer(t5_input, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=max_answer_tokens,\n",
        "            do_sample=False\n",
        "        )\n",
        "    answer = tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "    # 13. Append source info to answer\n",
        "    if source_info:\n",
        "        sources = set()\n",
        "        review_info = None\n",
        "\n",
        "        for idx in ranked_indices[:used_k]:\n",
        "            doc = corpus[idx]\n",
        "            src = doc.get(\"source_url\", doc.get(\"source\", \"Unknown\"))\n",
        "            if src:\n",
        "                sources.add(src)\n",
        "\n",
        "            if not review_info:\n",
        "                review_info = doc.get(\"review_info\", None)\n",
        "\n",
        "        if sources:\n",
        "            answer += \"\\n\\nFor more information visit:\"\n",
        "            for src in sources:\n",
        "                answer += f\"\\n {src}\"\n",
        "\n",
        "        if isinstance(review_info, str) and \"Page last reviewed:\" in review_info:\n",
        "            reviewed_part = review_info.split(\"Page last reviewed:\")[1].split(\"Next review due\")[0].strip()\n",
        "            answer += f\"\\n\\nInformation up-to-date as of: {reviewed_part}\"\n",
        "\n",
        "    return answer, retrieved_texts, boosted_used, ranked_indices[:used_k]\n"
      ],
      "metadata": {
        "id": "MLyK2MKp0nNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_pipeline_lora_top1title_ragseq(\n",
        "    query,\n",
        "    top_k=600,\n",
        "    bonus_weight=0.1,\n",
        "    max_answer_tokens=128,\n",
        "    confidence_threshold=0.63,\n",
        "    top_m=5,\n",
        "    alpha=0.85,\n",
        "    source_info=False,\n",
        "    return_chosen=False\n",
        "):\n",
        "\n",
        "    # Normalized log prob calculator\n",
        "    def _length_norm_seq_logprob(gen_out):\n",
        "        scores = gen_out.scores\n",
        "        if not scores: return float(\"-inf\")\n",
        "        chosen = gen_out.sequences[0][-len(scores):]\n",
        "        total = 0.0\n",
        "        for step, logits in enumerate(scores):\n",
        "            lp = torch.log_softmax(logits[0], dim=-1)\n",
        "            total += float(lp[int(chosen[step].item())])\n",
        "        return total / max(1, len(scores))\n",
        "\n",
        "    # 1. Normalize query\n",
        "    normalized_query, matched_main_terms, _ = cui_normalization(query)\n",
        "    if not matched_main_terms:\n",
        "        return \"!Please rephrase the query using a specific medical condition!\", [], [], None\n",
        "\n",
        "    # 2. Title match indices\n",
        "    heading_matched_indices = [\n",
        "        i for i, doc_text in enumerate(normalized_for_index)\n",
        "        if any(term in doc_text.split('.')[0].lower() for term in matched_main_terms)\n",
        "    ]\n",
        "\n",
        "    # 3. FAISS search\n",
        "    inputs = q_tokenizer(normalized_query, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        q_emb = q_encoder(**inputs).pooler_output.cpu().numpy()\n",
        "        q_emb /= np.linalg.norm(q_emb, axis=1, keepdims=True)\n",
        "\n",
        "    # 3.1 Scores and indices for the top documents\n",
        "    tk = min(int(top_k), index.ntotal)\n",
        "    scores, indices = index.search(q_emb, tk)\n",
        "    scores, indices = scores[0], indices[0]\n",
        "    valid = [(int(i), float(s)) for i, s in zip(indices, scores) if 0 <= i < len(corpus_texts)]\n",
        "    index_scores = {i: s for i, s in valid}\n",
        "\n",
        "    # 4. TF-IDF similarity score\n",
        "    tfidf_query_vec = tfidf_vectorizer.transform([normalized_query])\n",
        "    tfidf_query_vec = normalize(tfidf_query_vec)\n",
        "    tfidf_scores = (tfidf_matrix @ tfidf_query_vec.T).toarray().ravel()\n",
        "\n",
        "    # 5. Combine indices from FAISS and Title-matched docs\n",
        "    all_indices = set(index_scores.keys()) | set(heading_matched_indices)\n",
        "    for idx in all_indices:\n",
        "        if idx not in index_scores:\n",
        "            index_scores[idx] = 0.1\n",
        "\n",
        "    # 6. Combine DPR and TF-IDF scores\n",
        "    boosted_index_scores = {}\n",
        "    for idx in all_indices:\n",
        "        heading_match_count = count_heading_term_matches(normalized_for_index[idx], matched_main_terms)\n",
        "        dense_score  = float(index_scores.get(idx, 0.1))\n",
        "        sparse_score = float(tfidf_scores[idx])\n",
        "        boosted_index_scores[idx] = 0.9 * dense_score + 0.1 * sparse_score + bonus_weight * heading_match_count\n",
        "\n",
        "    # 7. Sorting scores and storing scores/indices for debugging\n",
        "    sorted_items   = sorted(boosted_index_scores.items(), key=lambda x: -x[1])\n",
        "    ranked_indices = [int(i) for i, _ in sorted_items]\n",
        "    sorted_scores  = [float(s) for _, s in sorted_items]\n",
        "\n",
        "    # 8. Retrieving top-M documents\n",
        "    M = min(int(top_m), len(ranked_indices))\n",
        "    retrieved_texts = [corpus_texts[idx] for idx in ranked_indices[:M]]\n",
        "    boosted_used    = sorted_scores[:M]\n",
        "\n",
        "    # 9. Threshold-based confidence check\n",
        "    if all(s < confidence_threshold for s in boosted_used):\n",
        "        return \"!I do not have enough confidence to answer your question.\", retrieved_texts, boosted_used, None\n",
        "\n",
        "    # 10. Generation per document\n",
        "    best = {\"score\": float(\"-inf\"), \"answer\": None, \"doc_idx\": None}\n",
        "\n",
        "    for i in range(M):\n",
        "        doc_idx   = ranked_indices[i]\n",
        "        doc_score = sorted_scores[i]\n",
        "        if doc_score < confidence_threshold:\n",
        "            continue\n",
        "\n",
        "        ctx_text = corpus_texts[doc_idx].strip()\n",
        "        t5_input = f\"question: {query}\\ncontext: {ctx_text}\"\n",
        "\n",
        "        enc = tokenizer(t5_input, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model.generate(\n",
        "                **enc,\n",
        "                max_new_tokens=max_answer_tokens,\n",
        "                do_sample=False,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "            )\n",
        "\n",
        "        seq = tokenizer.decode(out.sequences[0], skip_special_tokens=True).strip()\n",
        "        seq_lp = _length_norm_seq_logprob(out)\n",
        "\n",
        "        # blended scoring + short-answer penalty\n",
        "        length_penalty = -2.0 if len(seq.split()) < 3 else 0.0\n",
        "        combined = alpha * seq_lp + (1 - alpha) * doc_score + length_penalty\n",
        "\n",
        "        if combined > best[\"score\"]:\n",
        "            best.update({\"score\": combined, \"answer\": seq, \"doc_idx\": doc_idx})\n",
        "\n",
        "    # 11. Append source info\n",
        "    if source_info and best[\"doc_idx\"] is not None:\n",
        "        doc = corpus[best[\"doc_idx\"]]\n",
        "        src = doc.get(\"source_url\", doc.get(\"source\", \"Unknown\"))\n",
        "        review_info = doc.get(\"review_info\", None)\n",
        "\n",
        "        if src:\n",
        "            best[\"answer\"] += f\"\\n\\nFor more information visit:\\n {src}\"\n",
        "        if isinstance(review_info, str) and \"Page last reviewed:\" in review_info:\n",
        "            reviewed_part = review_info.split(\"Page last reviewed:\")[1].split(\"Next review due\")[0].strip()\n",
        "            best[\"answer\"] += f\"\\n\\nInformation up-to-date as of: {reviewed_part}\"\n",
        "\n",
        "    if return_chosen:\n",
        "        return (\n",
        "            best[\"answer\"],\n",
        "            ranked_indices[:M],\n",
        "            boosted_used,\n",
        "            best[\"doc_idx\"],\n",
        "        )\n",
        "    else:\n",
        "        return best[\"answer\"], ranked_indices[:M], boosted_used, best[\"doc_idx\"]\n"
      ],
      "metadata": {
        "id": "lVY6_djn1K9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_models(query):\n",
        "    # Run your concat pipeline\n",
        "    ans_concat, retrieved_concat, scores_concat, idx_concat = rag_pipeline_lora_top1title_concat(\n",
        "        query, source_info=True\n",
        "    )\n",
        "\n",
        "    # Run your ragseq pipeline\n",
        "    ans_ragseq, retrieved_ragseq, scores_ragseq, idx_ragseq = rag_pipeline_lora_top1title_ragseq(\n",
        "        query, source_info=True\n",
        "    )\n",
        "\n",
        "    # Format nicely\n",
        "    output = f\"**Concat Pipeline Answer:**\\n{ans_concat}\\n\\n---\\n\\n\"\n",
        "    output += f\"**RagSeq Pipeline Answer:**\\n{ans_ragseq}\"\n",
        "\n",
        "    return output\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## RAG LoRA App\\nAsk your question containing a medical condition and see answers from both pipelines.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            query = gr.Textbox(label=\"Your Medical Query\", placeholder=\"Type your medical query here.\")\n",
        "            btn = gr.Button(\"Send\")\n",
        "        with gr.Column():\n",
        "            output = gr.Markdown()\n",
        "\n",
        "    btn.click(fn=ask_models, inputs=query, outputs=output)\n",
        "\n",
        "demo.launch(share=True, quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "M-HxgAVA25Pe",
        "outputId": "3870abf2-4881-41e4-fbed-0e8d72978e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Running on public URL: https://9f1196debc1519eba6.gradio.live\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9f1196debc1519eba6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}