{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO8wwA1fs1DsHaAo3UnD5iw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danny2173/RAGproject/blob/main/4_Term_Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "McSaXfd9Foz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "%pip install -q pandas nltk\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Imports\n",
        "import json, re, pickle\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "Xzef7BJlTXnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Corpus"
      ],
      "metadata": {
        "id": "GRSAknZlFrTg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y_VegNpScKk",
        "outputId": "fb2469ff-557d-4f7e-a1bf-39b59fd1e52c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Corpus loaded\n"
          ]
        }
      ],
      "source": [
        "load_path = '/content/drive/MyDrive/corpus.json'\n",
        "\n",
        "# Load the corpus\n",
        "with open(load_path, \"r\") as f:\n",
        "    corpus = json.load(f)\n",
        "\n",
        "print(\"Corpus loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building term normalization"
      ],
      "metadata": {
        "id": "Bg5swOvwFxaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Loading MRCONSO.RRF\n",
        "mrconso_path = '/content/drive/MyDrive/MRCONSO.RRF'\n",
        "col_names = [\n",
        "    \"CUI\", \"LAT\", \"TS\", \"LUI\", \"STT\", \"SUI\", \"ISPREF\", \"AUI\",\n",
        "    \"SAUI\", \"SCUI\", \"SDUI\", \"SAB\", \"TTY\", \"CODE\", \"STR\",\n",
        "    \"SRL\", \"SUPPRESS\", \"CVF\"\n",
        "]\n",
        "\n",
        "# Filter for only English and MeSH dictionary\n",
        "df = pd.read_csv(mrconso_path, sep='|', names=col_names, dtype=str, index_col=False)\n",
        "df = df[(df[\"LAT\"] == \"ENG\")]\n",
        "df = df[df[\"SAB\"].isin([\"MSH\", \"SNOMEDCT_US\", \"LCH\", \"MEDLINE\", \"NCI\"])]\n",
        "df = df.dropna(subset=[\"STR\"])\n",
        "\n",
        "# Loading MRSTY.RRF for merging label type\n",
        "sty_df = pd.read_csv(\n",
        "    '/content/drive/MyDrive/MRSTY.RRF',\n",
        "    sep='|',\n",
        "    header=None,\n",
        "    usecols=[0, 1, 3],\n",
        "    names=['CUI', 'TUI', 'STY'],\n",
        "    dtype=str\n",
        ")\n",
        "\n",
        "\n",
        "# Selecting MH (Main Heading) as the main term (priority order)\n",
        "tty_order = [\"MH\", \"PM\", \"SY\", \"ENTRY\"]\n",
        "\n",
        "def best_label(group):\n",
        "    for tty in tty_order:\n",
        "        matches = group[group[\"TTY\"] == tty]\n",
        "        if not matches.empty:\n",
        "            return matches.iloc[0][\"STR\"].lower()\n",
        "    return group.iloc[0][\"STR\"].lower()\n",
        "\n",
        "# Grouping by CUI key\n",
        "cui_to_main_term = (\n",
        "    df.groupby(\"CUI\", group_keys=False)\n",
        "    .apply(best_label)\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# Function to find synonyms\n",
        "cui_to_synonyms = defaultdict(set)\n",
        "for _, row in df.iterrows():\n",
        "    cui_to_synonyms[row[\"CUI\"]].add(row[\"STR\"].lower())\n",
        "\n",
        "# Merge together using main term and synonym df\n",
        "merged = sty_df[sty_df[\"CUI\"].isin(cui_to_main_term)].copy()\n",
        "merged[\"Preferred\"] = merged[\"CUI\"].map(cui_to_main_term)\n",
        "merged[\"Synonyms\"] = merged[\"CUI\"].map(lambda cui: sorted(cui_to_synonyms[cui]))\n",
        "\n",
        "# Filtering for Disease/Condition only\n",
        "allowed_tuis = {\"T047\", \"T191\"}\n",
        "filtered = merged[merged[\"TUI\"].isin(allowed_tuis)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtUTTi15p6U8",
        "outputId": "ea8a361e-f469-4b73-fbda-54e5a496008c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-32622702.py:39: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(get_best_label_from_group)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracking CUIs wanted\n",
        "allowed_cuis = set(filtered[\"CUI\"])\n",
        "\n",
        "# Rebuilding cui_to_synonyms and cui_to_main_term using allowed CUIs\n",
        "cui_to_synonyms = defaultdict(set)\n",
        "for _, row in df[df[\"CUI\"].isin(allowed_cuis)].iterrows():\n",
        "    cui_to_synonyms[row[\"CUI\"]].add(row[\"STR\"].lower())\n",
        "\n",
        "cui_to_main_term = (\n",
        "    df[df[\"CUI\"].isin(allowed_cuis)]\n",
        "    .groupby(\"CUI\", group_keys=False)\n",
        "    .apply(get_best_label_from_group)\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# Reverse look-up synonym -> CUI\n",
        "term_to_CUI = {\n",
        "    syn: cui for cui, syns in cui_to_synonyms.items() for syn in syns\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgnvCfJ_sIbs",
        "outputId": "5fc1df4e-6690-45cf-bf6d-433bbd31a7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-441290011.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(get_best_label_from_group)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving Pickle files"
      ],
      "metadata": {
        "id": "N4Zja3k_aUZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving pickle files\n",
        "with open(\"/content/drive/MyDrive/filtered_term_to_CUI.pkl\", \"wb\") as f:\n",
        "    pickle.dump(term_to_CUI, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/filtered_cui_to_main_term.pkl\", \"wb\") as f:\n",
        "    pickle.dump(cui_to_main_term, f)\n"
      ],
      "metadata": {
        "id": "4J-yrU31yg1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading pickle files\n",
        "\n",
        "with open(\"/content/drive/MyDrive/filtered_term_to_CUI.pkl\", \"rb\") as f:\n",
        "    term_to_CUI = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/filtered_cui_to_main_term.pkl\", \"rb\") as f:\n",
        "    cui_to_main_term = pickle.load(f)"
      ],
      "metadata": {
        "id": "8DuDBwIdy33J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normalizing Corpus"
      ],
      "metadata": {
        "id": "fa8NK_zyF4vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating ngrams and tracking indices\n",
        "def ngram_tokenize_tokens(tokens, max_len=5):\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens)):\n",
        "        for j in range(i + 1, min(i + 1 + max_len, len(tokens) + 1)):\n",
        "            span = tokens[i:j]\n",
        "            ngram = ' '.join(span)\n",
        "            ngrams.append((ngram, i, j))\n",
        "    return ngrams\n",
        "\n",
        "# Normalizing medical terms using main condition name\n",
        "def cui_normalization(sentence, max_ngram_len=5):\n",
        "    tokens = re.findall(r'\\w+|\\W+', sentence)\n",
        "    # Filtering out words\n",
        "    words = [w.lower() for w in tokens if re.match(r'\\w+', w)]\n",
        "\n",
        "    # Call tokenization function to return ngrams tuples\n",
        "    ngrams = ngram_tokenize_tokens(words, max_ngram_len)\n",
        "    replacements = []\n",
        "\n",
        "    # Searching for terms in dictionary\n",
        "    for ngram, start, end in ngrams:\n",
        "        if ngram in term_to_CUI:\n",
        "            cui = term_to_CUI[ngram]\n",
        "            if cui in cui_to_main_term:\n",
        "                replacements.append((start, end, cui_to_main_term[cui]))\n",
        "\n",
        "    # Sorting by length then index (ensure longer terms first)\n",
        "    replacements.sort(key=lambda x: (x[0], -(x[1] - x[0])))\n",
        "    used = set()\n",
        "    final = []\n",
        "    # Ensure no overlap (check already used indices)\n",
        "    for start, end, main_term in replacements:\n",
        "        if not any(i in used for i in range(start, end)):\n",
        "            final.append((start, end, main_term))\n",
        "            used.update(range(start, end))\n",
        "\n",
        "    # Reconstruct the sentence\n",
        "    word_idx = 0\n",
        "    output = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        # If the token is a word\n",
        "        if re.match(r'\\w+', tokens[i]):\n",
        "            # Checking if index appears in final\n",
        "            match = next((f for f in final if f[0] == word_idx), None)\n",
        "            if match:\n",
        "                output.append(match[2])  # append main term\n",
        "                skip = match[1] - match[0]\n",
        "                while skip > 0 and i < len(tokens):\n",
        "                    if re.match(r'\\w+', tokens[i]):\n",
        "                        skip -= 1\n",
        "                    i += 1\n",
        "                # Update word-level index\n",
        "                word_idx += (match[1] - match[0])\n",
        "                continue\n",
        "            word_idx += 1\n",
        "        output.append(tokens[i])\n",
        "        i += 1\n",
        "\n",
        "    return ''.join(output)\n"
      ],
      "metadata": {
        "id": "rgVRDWbwTFaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example"
      ],
      "metadata": {
        "id": "JHY43zB1abDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input sentence\n",
        "example = \"What are the symptons of type 2 diabetes?\"\n",
        "\n",
        "# Normalize it\n",
        "normalized = cui_normalization(example)\n",
        "\n",
        "# Output\n",
        "print(\"Original:\", example)\n",
        "print(\"Normalized:\", normalized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-qrxy7iaO_h",
        "outputId": "603b6529-688e-47a1-b505-766e20cf161a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: What are the symptons of type 2 diabetes?\n",
            "Normalized: What are the symptons of diabetes mellitus, type 2?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing entire corpus\n",
        "\n",
        "for entry in corpus:\n",
        "    entry[\"normalized_text\"] = cui_normalization(entry[\"text\"])\n"
      ],
      "metadata": {
        "id": "pcXY79B_Wq_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting Corpus"
      ],
      "metadata": {
        "id": "UILOHAk5agKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting Corpus\n",
        "\n",
        "save_path = '/content/drive/MyDrive/corpus.json'\n",
        "\n",
        "with open(save_path, \"w\") as f:\n",
        "    json.dump(corpus, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Corpus saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0X_4l8l05_K",
        "outputId": "f07f0733-b1d5-4635-c512-459bb4342fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus saved to /content/drive/MyDrive/corpus.json\n"
          ]
        }
      ]
    }
  ]
}