{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMA5ZjP3WNZPLVL9Vt5Ko+I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danny2173/RAGproject/blob/main/5_Retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports and Installs"
      ],
      "metadata": {
        "id": "5C0bvAwydYBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "%pip install -q transformers faiss-cpu tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Imports\n",
        "import os, json, re, pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozMMG38rmhx2",
        "outputId": "c4c781e2-1076-4536-8be4-b954838e482f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Corpus"
      ],
      "metadata": {
        "id": "Vq1JNN_oddGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzQlBE_MmPif",
        "outputId": "e8bb520b-f38c-4ec4-e212-12d4064bb970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus loaded\n"
          ]
        }
      ],
      "source": [
        "load_path = '/content/drive/MyDrive/corpus.json'\n",
        "\n",
        "# Load the corpus\n",
        "with open(load_path, \"r\") as f:\n",
        "    corpus = json.load(f)\n",
        "\n",
        "print(\"Corpus loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formatting for retriever understanding"
      ],
      "metadata": {
        "id": "VWORUVBQdhZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def format_text(text, merge_bullets=True):\n",
        "    text = re.sub(r'(\\.\\s{2,})([a-zA-Z])', r'\\1\\n\\2', text)\n",
        "\n",
        "    # Removing Non-urgent advice heading\n",
        "    text = re.sub(r'^Subsection: Non-urgent advice:\\s*', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Fix: Convert colon to period if next line is Section/Subsection/Subsubsection\n",
        "    text = re.sub(\n",
        "        r'(:)\\n(Section:|Subsection:|Subsubsection:)',\n",
        "        r'.\\n\\2',\n",
        "        text\n",
        "    )\n",
        "\n",
        "    def bullet_merge_safely(match):\n",
        "        parts = [s.strip() for s in re.split(r'\\s{2,}', match.group(3)) if s.strip()]\n",
        "        merged_parts = []\n",
        "        for part in parts:\n",
        "            if part.endswith('.'):\n",
        "                # If this part is a full sentence, break it to a new line\n",
        "                merged_parts.append(part + \"\\n\")\n",
        "            else:\n",
        "                merged_parts.append(part)\n",
        "\n",
        "        # Merge non-sentence-ending items into a sentence\n",
        "        bullet_items = [p for p in merged_parts if not p.endswith('\\n')]\n",
        "\n",
        "        # Ensure the last bullet item ends with a period\n",
        "        if bullet_items and not bullet_items[-1].endswith('.'):\n",
        "            bullet_items[-1] += '.'\n",
        "\n",
        "        sentence = \"\"\n",
        "        if bullet_items:\n",
        "            sentence = \", \".join(bullet_items[:-1]) + \", and \" + bullet_items[-1] if len(bullet_items) > 1 else bullet_items[0]\n",
        "\n",
        "        # Join sentence part and line-break part\n",
        "        result = match.group(1) + ' ' + sentence\n",
        "        for part in merged_parts:\n",
        "            if part.endswith('\\n'):\n",
        "                result += \"\\n\" + part.strip()\n",
        "        return result\n",
        "\n",
        "    # Apply bullet merging\n",
        "    text = re.sub(r'(:)(\\s{2,})([^\\n]+)', bullet_merge_safely, text)\n",
        "\n",
        "    lines = text.splitlines()\n",
        "    output = []\n",
        "    buffer = []\n",
        "    current_header = None\n",
        "    in_do_section = False  # Flag to check if we're in a \"Do\" block\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Detect entering \"Do\" section\n",
        "        if \"subsubsection:\" in line.lower() and \"do\" in line.lower():\n",
        "            if buffer:\n",
        "                if merge_bullets and current_header:\n",
        "                    sentence = f\"{current_header.rstrip(':').strip()} \" + (\n",
        "                        \", \".join(buffer[:-1]) + \", and \" + buffer[-1] if len(buffer) > 1 else buffer[0]\n",
        "                    ) + \".\"\n",
        "                    output.append(sentence)\n",
        "                else:\n",
        "                    output.extend([f\"- {item}\" for item in buffer])\n",
        "                buffer = []\n",
        "            current_header = None\n",
        "            in_do_section = True\n",
        "            continue\n",
        "\n",
        "        if line.startswith((\"Section:\", \"Subsection:\", \"Subsubsection:\")):\n",
        "            if buffer:\n",
        "                if merge_bullets and current_header and not in_do_section:\n",
        "                    sentence = f\"{current_header.rstrip(':').strip()} \" + (\n",
        "                        \", \".join(buffer[:-1]) + \", and \" + buffer[-1] if len(buffer) > 1 else buffer[0]\n",
        "                    ) + \".\"\n",
        "                    output.append(sentence)\n",
        "                else:\n",
        "                    output.extend([f\"- {item}\" for item in buffer])\n",
        "                buffer = []\n",
        "                current_header = None\n",
        "\n",
        "            output.append(line)\n",
        "            continue\n",
        "\n",
        "        if line.endswith(\":\") and not line.startswith(\"-\") and not line.lower().strip() in [\"do:\", \"don't:\"]:\n",
        "            if buffer:\n",
        "                if merge_bullets and current_header:\n",
        "                    sentence = f\"{current_header.rstrip(':').strip()} \" + (\n",
        "                        \", \".join(buffer[:-1]) + \", and \" + buffer[-1] if len(buffer) > 1 else buffer[0]\n",
        "                    ) + \".\"\n",
        "                    output.append(sentence)\n",
        "                else:\n",
        "                    output.extend([f\"- {item}\" for item in buffer])\n",
        "                buffer = []\n",
        "\n",
        "            current_header = line\n",
        "            continue\n",
        "\n",
        "        if line.startswith(\"- \"):\n",
        "            bullet_content = line[2:].strip()\n",
        "            subitems = re.split(r'\\s{2,}', bullet_content)\n",
        "            buffer.extend([s.strip() for s in subitems if s.strip()])\n",
        "            continue\n",
        "\n",
        "        # Normal paragraph lines\n",
        "        if buffer:\n",
        "            if merge_bullets and current_header:\n",
        "                sentence = f\"{current_header.rstrip(':').strip()} \" + (\n",
        "                    \", \".join(buffer[:-1]) + \", and \" + buffer[-1] if len(buffer) > 1 else buffer[0]\n",
        "                ) + \".\"\n",
        "                output.append(sentence)\n",
        "            else:\n",
        "                output.extend([f\"- {item}\" for item in buffer])\n",
        "            buffer = []\n",
        "            current_header = None\n",
        "\n",
        "        output.append(line)\n",
        "\n",
        "    # Final buffer flush\n",
        "    if buffer:\n",
        "        if in_do_section:\n",
        "            sentence = \"Do \" + (\n",
        "                \", \".join(buffer[:-1]) + \", and \" + buffer[-1] if len(buffer) > 1 else buffer[0]\n",
        "            ) + \".\"\n",
        "            output.append(sentence)\n",
        "        elif merge_bullets and current_header:\n",
        "            sentence = f\"{current_header.rstrip(':').strip()} \" + (\n",
        "                \", \".join(buffer[:-1]) + \", and \" + buffer[-1] if len(buffer) > 1 else buffer[0]\n",
        "            ) + \".\"\n",
        "            output.append(sentence)\n",
        "        else:\n",
        "            output.extend([f\"- {item}\" for item in buffer])\n",
        "\n",
        "    # Ensure headings end with a period\n",
        "    final_text = \"\\n\".join(output)\n",
        "    final_text = re.sub(r'^(Section:[^\\n]*?)(?<!\\.)$', r'\\1.', final_text, flags=re.MULTILINE)\n",
        "    final_text = re.sub(r'^(Subsection:[^\\n]*?)(?<!\\.)$', r'\\1.', final_text, flags=re.MULTILINE)\n",
        "    final_text = re.sub(r'^(Subsubsection:[^\\n]*?)(?<!\\.)$', r'\\1.', final_text, flags=re.MULTILINE)\n",
        "    # Remove excess colons\n",
        "    final_text = re.sub(r'(?<!Section)(?<!Subsection)(?<!Subsubsection):', '', final_text)\n",
        "\n",
        "    # Ensure each sentence is on its own line\n",
        "    final_text = re.sub(r'([.?!])\\s+', r'\\1\\n', final_text)\n",
        "\n",
        "    return final_text\n",
        "\n",
        "for doc in corpus:\n",
        "    # Keep original structure for Generator\n",
        "    doc[\"text\"] = format_text(doc[\"text\"], merge_bullets=False)\n",
        "    # Apply merge_bullets function to normalized text version\n",
        "    doc[\"normalized_text\"] = format_text(doc[\"normalized_text\"], merge_bullets=True)\n"
      ],
      "metadata": {
        "id": "bkrats5TmkZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test"
      ],
      "metadata": {
        "id": "kcUOJ7SEnFDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus[2][\"normalized_text\"])\n",
        "print(\"-\"*100)\n",
        "print(corpus[2][\"normalized_text\"])\n",
        "print(\"-\"*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "272qBe2bPVwe",
        "outputId": "e3f19921-3a8d-4c2f-f1db-094f4a751dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Section: esophageal achalasia.\n",
            "esophageal achalasia is a rare disorder of the food pipe (oesophagus), which can make it difficult to swallow food and drink.\n",
            "Normally, the muscles of the oesophagus contract to squeeze food along towards the stomach.\n",
            "A ring of muscle at the end of the food pipe then relaxes to let food into the stomach.\n",
            "In esophageal achalasia, the muscles in the oesophagus do not contract correctly and the ring of muscle can fail to open properly, or does not open at all.\n",
            "Food and drink cannot pass into the stomach and becomes stuck.\n",
            "It is often brought back up.\n",
            "Section: esophageal achalasia.\n",
            "Subsection: Symptoms of esophageal achalasia.\n",
            "Not everyone with esophageal achalasia will have symptoms.\n",
            "But most people with esophageal achalasia will find it difficult to swallow food or drink (known as deglutition disorders ).\n",
            "Swallowing tends to get gradually more difficult or painful over a couple of years, to the point where it is sometimes impossible.\n",
            "Other symptoms include bringing back up undigested food, choking and coughing fits, heartburn, chest pain, repeated chest infections, drooling of vomit or saliva, and gradual but significant weight loss.\n",
            "Symptoms of esophageal achalasia may start at any time of life.\n",
            "Swallowing problems can also be caused by cancers of the mouth, throat and oesophagus.\n",
            "There’s a link between long-term esophageal achalasia and the risk of developing cancer of the oesophagus, but the risk is small.\n",
            "It’s always important to get symptoms checked straight away, even if your symptoms are not bothering you.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Section: esophageal achalasia.\n",
            "esophageal achalasia is a rare disorder of the food pipe (oesophagus), which can make it difficult to swallow food and drink.\n",
            "Normally, the muscles of the oesophagus contract to squeeze food along towards the stomach.\n",
            "A ring of muscle at the end of the food pipe then relaxes to let food into the stomach.\n",
            "In esophageal achalasia, the muscles in the oesophagus do not contract correctly and the ring of muscle can fail to open properly, or does not open at all.\n",
            "Food and drink cannot pass into the stomach and becomes stuck.\n",
            "It is often brought back up.\n",
            "Section: esophageal achalasia.\n",
            "Subsection: Symptoms of esophageal achalasia.\n",
            "Not everyone with esophageal achalasia will have symptoms.\n",
            "But most people with esophageal achalasia will find it difficult to swallow food or drink (known as deglutition disorders ).\n",
            "Swallowing tends to get gradually more difficult or painful over a couple of years, to the point where it is sometimes impossible.\n",
            "Other symptoms include bringing back up undigested food, choking and coughing fits, heartburn, chest pain, repeated chest infections, drooling of vomit or saliva, gradual but significant weight loss, Symptoms of esophageal achalasia may start at any time of life., Swallowing problems can also be caused by cancers of the mouth, throat and oesophagus.\n",
            "There’s a link between long-term esophageal achalasia and the risk of developing cancer of the oesophagus, but the risk is small., and It’s always important to get symptoms checked straight away, even if your symptoms are not bothering you.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Section: Mouth ulcers\n",
            "Mouth ulcers are common and should clear up on their own within a week or 2. But see a GP or dentist if you have a mouth ulcer that lasts longer than 3 weeks.\n",
            "Section: Mouth ulcers\n",
            "Subsection: How you can treat mouth ulcers yourself\n",
            "Mouth ulcers are rarely a sign of anything serious, but may be uncomfortable to live with.  They need time to heal and there's no quick fix.  Avoiding things that irritate your mouth ulcer should help:  speed up the healing process  reduce pain  reduce the chance of it returning\n",
            "Section: Mouth ulcers\n",
            "Subsection: How you can treat mouth ulcers yourself\n",
            "Subsubsection: Do\n",
            "- use a soft-bristled toothbrush  drink cool drinks through a straw  eat softer foods  get regular dental check-ups  eat a healthy, balanced diet\n",
            "Section: Mouth ulcers\n",
            "Subsection: How you can treat mouth ulcers yourself\n",
            "Subsubsection: Don’t\n",
            "- do not eat very spicy, salty or acidic food  do not eat rough, crunchy food, such as toast or crisps  do not drink very hot or acidic drinks, such as fruit juice  do not use toothpaste containing sodium lauryl sulphate\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Section: oral ulcer\n",
            "oral ulcer are common and should clear up on their own within a week or 2. But see a GP or dentist if you have a oral ulcer that lasts longer than 3 weeks.\n",
            "Section: oral ulcer\n",
            "Subsection: How you can treat oral ulcer yourself\n",
            "oral ulcer are rarely a sign of anything serious, but may be uncomfortable to live with.  They need time to heal and there's no quick fix.  Avoiding things that irritate your oral ulcer should help:  speed up the healing process  reduce pain  reduce the chance of it returning\n",
            "Section: oral ulcer\n",
            "Subsection: How you can treat oral ulcer yourself\n",
            "Subsubsection: Do\n",
            "- use a soft-bristled toothbrush  drink cool drinks through a straw  eat softer foods  get regular dental check-ups  eat a healthy, balanced diet\n",
            "Section: oral ulcer\n",
            "Subsection: How you can treat oral ulcer yourself\n",
            "Subsubsection: Don’t\n",
            "- do not eat very spicy, salty or acidic food  do not eat rough, crunchy food, such as toast or crisps  do not drink very hot or acidic drinks, such as fruit juice  do not use toothpaste containing sodium lauryl sulphate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Term Normalization functions"
      ],
      "metadata": {
        "id": "fribjmQWbAvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing conversion functions\n",
        "\n",
        "with open(\"/content/drive/MyDrive/filtered_term_to_CUI.pkl\", \"rb\") as f:\n",
        "    term_to_CUI = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/filtered_cui_to_main_term.pkl\", \"rb\") as f:\n",
        "    cui_to_main_term = pickle.load(f)\n",
        "\n",
        "# Creating ngrams and tracking indices\n",
        "def ngram_tokenize_tokens(tokens, max_len=5):\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens)):\n",
        "        for j in range(i + 1, min(i + 1 + max_len, len(tokens) + 1)):\n",
        "            span = tokens[i:j]\n",
        "            ngram = ' '.join(span)\n",
        "            ngrams.append((ngram, i, j))\n",
        "    return ngrams\n",
        "\n",
        "# Normalizing medical terms using main condition name\n",
        "def cui_normalization(sentence, max_ngram_len=5):\n",
        "    tokens = re.findall(r'\\w+|\\W+', sentence)\n",
        "\n",
        "    # Filtering out words\n",
        "    words = [w.lower() for w in tokens if re.match(r'\\w+', w)]\n",
        "\n",
        "    # Call tokenization function to return ngrams tuples\n",
        "    ngrams = ngram_tokenize_tokens(words, max_ngram_len)\n",
        "    replacements = []\n",
        "\n",
        "    # Tracking matched CUIs\n",
        "    matched_cuis = set()\n",
        "\n",
        "    # Searching for terms in dictionary\n",
        "    for ngram, start, end in ngrams:\n",
        "        if ngram in term_to_CUI:\n",
        "            cui = term_to_CUI[ngram]\n",
        "            if cui in cui_to_main_term:\n",
        "                replacements.append((start, end, cui_to_main_term[cui]))\n",
        "                matched_cuis.add(cui)\n",
        "\n",
        "    # Sorting by length then index (ensure longer terms first)\n",
        "    replacements.sort(key=lambda x: (x[0], -(x[1] - x[0])))\n",
        "    used = set()\n",
        "    final = []\n",
        "    # Ensure no overlap (check already used indices)\n",
        "    for start, end, main_term in replacements:\n",
        "        if not any(i in used for i in range(start, end)):\n",
        "            final.append((start, end, main_term))\n",
        "            used.update(range(start, end))\n",
        "\n",
        "    # Reconstruct the sentence\n",
        "    word_idx = 0\n",
        "    output = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        # If the token is a word\n",
        "        if re.match(r'\\w+', tokens[i]):\n",
        "            # Checking if index appears in final\n",
        "            match = next((f for f in final if f[0] == word_idx), None)\n",
        "            if match:\n",
        "                output.append(match[2]) # append main term\n",
        "                skip = match[1] - match[0]\n",
        "                while skip > 0 and i < len(tokens):\n",
        "                    if re.match(r'\\w+', tokens[i]):\n",
        "                        skip -= 1\n",
        "                    i += 1\n",
        "                # Update word-level index\n",
        "                word_idx += (match[1] - match[0])\n",
        "                continue\n",
        "            word_idx += 1\n",
        "        output.append(tokens[i])\n",
        "        i += 1\n",
        "\n",
        "    normalized_text = ''.join(output)\n",
        "    matched_main_terms = [term for _, _, term in final]\n",
        "    return normalized_text, matched_main_terms, list(matched_cuis)\n"
      ],
      "metadata": {
        "id": "6NNXz44LmuUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building FAISS Index"
      ],
      "metadata": {
        "id": "VfhRxLk1bK6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Loading dpr context encoder\n",
        "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "corpus_embeddings = []\n",
        "corpus_texts = []              # Retain for Generator\n",
        "normalized_for_index = []\n",
        "\n",
        "\n",
        "for item in tqdm(corpus, desc=\"Encoding corpus\"):\n",
        "\n",
        "    # Extracting title from id\n",
        "    raw_title = item.get(\"id\", \"\").split(\"_\")[0]\n",
        "    if not raw_title.endswith(\".\"):\n",
        "        raw_title += \".\"\n",
        "\n",
        "    # Appending title to raw text\n",
        "    raw_body = item.get(\"text\", \"\").strip()\n",
        "    corpus_texts.append(f\"{raw_title}\\n{raw_body}\")\n",
        "\n",
        "    # Normalize title and append to text\n",
        "    norm_title, _, _ = cui_normalization(raw_title)\n",
        "    norm_body = item.get(\"normalized_text\", \"\").strip()\n",
        "    full_text = f\"{norm_title}\\n{norm_body}\".strip()\n",
        "    normalized_for_index.append(full_text)\n",
        "\n",
        "    # Tokenize text\n",
        "    inputs = ctx_tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        emb = ctx_encoder(**inputs).pooler_output.squeeze().cpu().numpy()\n",
        "    # Append embedding\n",
        "    corpus_embeddings.append(emb)\n",
        "\n",
        "# Build FAISS index\n",
        "corpus_embeddings = np.asarray(corpus_embeddings, dtype=\"float32\")\n",
        "corpus_embeddings /= np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "index = faiss.IndexFlatIP(corpus_embeddings.shape[1])\n",
        "index.add(corpus_embeddings)\n",
        "\n",
        "print(f\"FAISS index built with {index.ntotal} vectors\")\n"
      ],
      "metadata": {
        "id": "aC5bQFWenEPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Saving"
      ],
      "metadata": {
        "id": "C7uuMxWezW4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save corpus_texts\n",
        "with open(\"/content/drive/MyDrive/corpus_texts.pkl\", \"wb\") as f:\n",
        "    pickle.dump(corpus_texts, f)\n",
        "\n",
        "# Save normalized_for_index\n",
        "with open(\"/content/drive/MyDrive/normalized_for_index.pkl\", \"wb\") as f:\n",
        "    pickle.dump(normalized_for_index, f)\n",
        "\n",
        "# Save raw corpus\n",
        "with open(\"/content/drive/MyDrive/corpus.pkl\", \"wb\") as f:\n",
        "    pickle.dump(corpus, f)\n",
        "\n",
        "# Save FAISS index\n",
        "faiss.write_index(index, \"/content/drive/MyDrive/faiss_index.index\")"
      ],
      "metadata": {
        "id": "bxKw5LDU5Gen"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}