{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4888,
     "status": "ok",
     "timestamp": 1755454444253,
     "user": {
      "displayName": "Dan Chappell",
      "userId": "18400027947275786982"
     },
     "user_tz": -60
    },
    "id": "OPMh176OOPMA",
    "outputId": "ca49c9fc-884f-4c22-eeb4-c82aac8f0f41"
   },
   "outputs": [],
   "source": [
    "!pip install nbstripout\n",
    "!nbstripout /content/drive/MyDriv3_Fine_tuning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37086,
     "status": "ok",
     "timestamp": 1755454294791,
     "user": {
      "displayName": "Dan Chappell",
      "userId": "18400027947275786982"
     },
     "user_tz": -60
    },
    "id": "JLqhJw4V0oG-",
    "outputId": "2d107469-0ed3-4b06-9376-312af1b6154d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sW5M3U3T2_2Q"
   },
   "outputs": [],
   "source": [
    "import os, gc, torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1194,
     "status": "ok",
     "timestamp": 1752823931441,
     "user": {
      "displayName": "Dan Chappell",
      "userId": "18400027947275786982"
     },
     "user_tz": -60
    },
    "id": "mGs3UExP0buz",
    "outputId": "9272f7d7-05b3-4243-e013-1f8b846ba3af"
   },
   "outputs": [],
   "source": [
    "load_path = \"/content/drive/MyDrive/expanded_dataset.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(load_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    expanded_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(expanded_data)} examples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "53a14f6e332f470baf0d4f59f409b55c",
      "1399b2b513d7490da82e39e205078a05",
      "5a23ed1231cb42c0a03285792965ac90",
      "e91aed5d89674f9aac887196e820cbfa",
      "950d11dc19404f6f907eae25b8dde464",
      "127dad7653b843afb7812814296fb21a",
      "936dbb75fa4c465bb09b731f1f6f2664",
      "1d03e2a96da34467ab95a2563881199e",
      "bbece43e0b694df3b55c6f42e2b4e6c3",
      "059d9f682a7b4507a57ba5c665b48988",
      "806c65bad9af455d9d07c1d8da8b90b2"
     ]
    },
    "executionInfo": {
     "elapsed": 2880996,
     "status": "ok",
     "timestamp": 1752827212975,
     "user": {
      "displayName": "Dan Chappell",
      "userId": "18400027947275786982"
     },
     "user_tz": -60
    },
    "id": "7AFIUlJKd6lB",
    "outputId": "d65c54c5-76d7-430f-b070-ca6b602ebfcc"
   },
   "outputs": [],
   "source": [
    "# Environmental setup - ensure sufficient memory\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data Preparation\n",
    "def flatten_text(text):\n",
    "    return text.replace('\\n', ' ').replace('  ', ' ').strip()\n",
    "\n",
    "# Creating input/output format\n",
    "reformatted_dataset = [\n",
    "    {\n",
    "        \"question\": item[\"question\"],\n",
    "        \"context\": flatten_text(item[\"context\"]),\n",
    "        \"answer\": item[\"answer\"]\n",
    "    }\n",
    "    for item in expanded_data\n",
    "]\n",
    "\n",
    "# Converting list to dataset\n",
    "dataset = Dataset.from_list(reformatted_dataset)\n",
    "\n",
    "# Tokenizing\n",
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "def tokenize_bart(example):\n",
    "    input_text = f\"Context: {example['context']} Question: {example['question']}\"\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        input_text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"answer\"],\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = [\n",
    "        label if label != tokenizer.pad_token_id else -100\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_bart, batched=False, remove_columns=dataset.column_names)\n",
    "\n",
    "# Setting up LoRA model\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Setting training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart-RAG\",\n",
    "    eval_strategy=\"no\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6fa5a5718f454d97b349239fd3b3b964",
      "d3351acc2b814f73a492c537ba2004a3",
      "4746c0ae807a497c8a2bb67cc5b22e0a",
      "78440e4303864cee90fa506ba7ac1b3c",
      "507f209c730e4ad48d24206c0a21b580",
      "1b83e01b5019479fad33ed57dee009ad",
      "71940777a1014c9da9b32074ddbd735f",
      "9aaa4aecec734ac48716af7632c644dc",
      "9f0b28f3122641d29e1d509e659afeaf",
      "cd15ce4a246e41068438aeff12d846d8",
      "ed2a616ec09645119781661095310983",
      "745eafe2e31540e4ad4e38a24964692d",
      "ba39f9a77ebc4bfdb2e07dd2a13640dc",
      "1b6531f6a12043c28a27c8c9121d9065",
      "51859daf8f45406087c05472061ba7f1",
      "96b34e0f5dc4447fb02d334080e8a02a",
      "b7c6f657f42a44378496a875129b7aea",
      "39864dd72eeb4535a6e292773af9807b",
      "2416309bf06844508d22649bb41cfe83",
      "dc1f0e982c9c43d5a138d156cb7991ff",
      "855c2ab56ea74370904b66118adc0e2f",
      "5b3eb4b53c204486828dc272d6e7e489",
      "7f81a3746adb4cf4b47573def3560dea",
      "ba2d11bfc07141a38da7a4fb6f8e2e8a",
      "3c645ec50e0f46bdb84e1d977fbd7e87",
      "5394092aa1e34c21b629db12788e1ebe",
      "9eb6eddb572e4600b4b27b14284669b9",
      "d48d3104e4d34934ab810d14fbb291a9",
      "c30bf94931f3463999c0923f2ef46bc0",
      "a9317fa99b6a4ed381e8c8a55368fc55",
      "3ac492c037af4454abc88b651ee8c3d3",
      "6746a1aba33e403abc363bf24414567e",
      "c4c7d2aff2854e37a38f83be12329496",
      "8920d3b4e3904d369bf992d36013d268",
      "4c83fdf7792846aa9a954fd257872509",
      "eca7f12acce540dbbd6ce6b4cd5c2445",
      "2158427718a0494ab8b11a3dbbad7a45",
      "4c0d0dfb31f743d994360c336c82c3f5",
      "fe934860310f43588609a96a0408fc45",
      "4eea6116ab62450ba2a9d774a255f698",
      "621eb17b34524da0923b5470f33c58df",
      "e849929ddf3148e09cc4ad3ba290af5c",
      "bbfe05d067b44ed68f8572e835b3779b",
      "19e4fa7976fb44dd98d91c73f98394ea",
      "82e90b4b552846c7a7c87a37d299c363",
      "de621843015b4ed7a41966b77c0e9d02",
      "1dfe1f1254284b0c9fd88a37a714b261",
      "5d9ee0dee09149e7a88b18a6049e27bb",
      "78d2af44c2304edf80bff1f30dc5a2f0",
      "8a44f76c5084473095746f638c40158f",
      "6984d7685b204598921552ae2af989d6",
      "51873e779b8346ba8de9b8b3b11d652f",
      "d766a136a60847ecbdcfd91faa08d222",
      "0df1c16059454612b6f1b8172e9ff951",
      "7edb95bd53094957ae6a58faff644984",
      "8ac9d330128c42029b9b2db11809c705",
      "996997982a9541b3a692fa697048640a",
      "2b966348479b47b6ab7df0d87ca9f065",
      "846f3d7807b84681a0d5cf56ba3c19ae",
      "25ac4babad524e128677979c79e284b5",
      "d84b1db521bd4a71bc81aa5ef70226e2",
      "8b85bc36c05043e797aeb250dfa6b267",
      "d7939dc539a64ef99088b16b489876a4",
      "075bd18f3d8245f6b0ed72e1bf687b52",
      "a9991bbc5192458889253f3e64ea4be6",
      "7d0dcf8a23594c22b3ca12cea1665b52"
     ]
    },
    "executionInfo": {
     "elapsed": 5680655,
     "status": "ok",
     "timestamp": 1752046748717,
     "user": {
      "displayName": "Dan Chappell",
      "userId": "18400027947275786982"
     },
     "user_tz": -480
    },
    "id": "OAMVj3DC26qI",
    "outputId": "150bc0db-5292-46dc-817d-2f41714d670e"
   },
   "outputs": [],
   "source": [
    "# Environmental setup - ensure sufficient memory\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Data Preparation\n",
    "\n",
    "def flatten_text(text):\n",
    "    return text.replace('\\n', ' ').replace('  ', ' ').strip()\n",
    "\n",
    "# Creating input/output format\n",
    "reformatted_dataset = [\n",
    "    {\n",
    "        \"input\": f\"question: {item['question']} context: {flatten_text(item['context'])}\",\n",
    "        \"output\": item[\"answer\"]\n",
    "    }\n",
    "    for item in expanded_data\n",
    "]\n",
    "\n",
    "# Converting list to dataset\n",
    "dataset = Dataset.from_list(reformatted_dataset)\n",
    "\n",
    "\n",
    "# Tokenizing\n",
    "model = \"t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "def tokenize(example):\n",
    "    # Tokenize input\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input\"],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    # Tokenize target (output)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"output\"],\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    # Transforming padded token positions\n",
    "    model_inputs[\"labels\"] = [\n",
    "        l if l != tokenizer.pad_token_id else -100 for l in labels[\"input_ids\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Setting up LoRA model\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Setting training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-RAG\",\n",
    "    eval_strategy=\"no\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Training\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1557,
     "status": "ok",
     "timestamp": 1752760253989,
     "user": {
      "displayName": "Dan Chappell",
      "userId": "18400027947275786982"
     },
     "user_tz": -60
    },
    "id": "8NRYR06-K7mb",
    "outputId": "9acbca7b-6389-4bd4-d8bf-fb7f4378c059"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"/content/drive/MyDrive/t5-lora-final\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/t5-lora-final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "0e2ac50b0c7c4a2582187a18edb90ef9",
      "8401a6fe64cd44ef972c1d8410d87c3a",
      "f22047a483654f1e8eb6910d8d1c77df",
      "f0d30fbac80a424086626de87294be6d",
      "b17ca3f47dc04a3ba218e170be72ba23",
      "a30da8f9f16e4ef99e274dce04dc4058",
      "fa301dafdaa64db7aded622319bc6fca",
      "15abae8f00684acab9a4c45f301ce9a7",
      "6b866c11b3d34c56b575e20f7d20adbe",
      "a4bdafbc00b4476dae2a7ee1bc11dab5",
      "a77c9fa6a89444db9cb8b809c3dbc514",
      "3948d983ccf5491f974f155b3e7812fc",
      "69fcae66255e4b6498654c942776c2b5",
      "cfc74261a0714d3b9486f9627962215d",
      "4f1af51133c44e828a41e41deaf35530",
      "cfb68d0a9af4485b9a5ff562af0af483",
      "945db8e6ded14767a437786877b32b08",
      "468c45a2b335404681517987c9ea4e62",
      "013f38d0943948eda983bbd86ead4825",
      "ada5fd3c00784629886be87e38158374",
      "8964fe44631c466ab3b774a1591f789a",
      "61e517e412cc461287b859f346864f5a",
      "6d6f709c73244cb48d38c846d2be7862",
      "787dbbf7060b405ebdc4122e07751303",
      "84d4ecc855c040309087ce7ff8e4ce11",
      "7825ff0c5cd341a5af027b95bcf24439",
      "a4b86b4238274d99bede9b38edd7727b",
      "500ddfe1338b4b618edda3331e74fa61",
      "3e82fc3f1b8c46a7a5dc1b48cde711c5",
      "6c8244cba4db470ab9eb1d5a23824c37",
      "8241a76d9d4947868655276ffd65df27",
      "a8f3346ecb714ac080008530ae78465d",
      "5922e03db6f44340a888bbcd3fbc8f76"
     ]
    },
    "executionInfo": {
     "elapsed": 130813,
     "status": "ok",
     "timestamp": 1752053140603,
     "user": {
      "displayName": "Dan Chappell",
      "userId": "18400027947275786982"
     },
     "user_tz": -480
    },
    "id": "9t_VEvjTLRUN",
    "outputId": "90d0bb45-7c9e-4451-f91e-fc28658ebe3e"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/t5-lora-final\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")\n",
    "model = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/t5-lora-final\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO+bvhVdsN3rxBQEYTFqE30",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
