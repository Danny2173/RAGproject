{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danny2173/RAGproject/blob/main/3_Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "BejgUrPoWj0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLqhJw4V0oG-"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q transformers datasets peft\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Imports\n",
        "import os, gc, json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Generated Question-Answer dataset"
      ],
      "metadata": {
        "id": "-qKe8F8AWuBn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGs3UExP0buz"
      },
      "outputs": [],
      "source": [
        "load_path = \"/content/drive/MyDrive/expanded_dataset.json\"\n",
        "\n",
        "# Load the JSON file\n",
        "with open(load_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    expanded_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(expanded_data)} examples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning (BART)"
      ],
      "metadata": {
        "id": "Zdt7g8x7W_9a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AFIUlJKd6lB"
      },
      "outputs": [],
      "source": [
        "# Environmental setup - ensure sufficient memory\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data Preparation\n",
        "def flatten_text(text):\n",
        "    return text.replace('\\n', ' ').replace('  ', ' ').strip()\n",
        "\n",
        "# Creating input/output format\n",
        "reformatted_dataset = [\n",
        "    {\n",
        "        \"question\": item[\"question\"],\n",
        "        \"context\": flatten_text(item[\"context\"]),\n",
        "        \"answer\": item[\"answer\"]\n",
        "    }\n",
        "    for item in expanded_data\n",
        "]\n",
        "\n",
        "# Converting list to dataset\n",
        "dataset = Dataset.from_list(reformatted_dataset)\n",
        "\n",
        "# Tokenizing\n",
        "model_name = \"facebook/bart-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "\n",
        "def tokenize_bart(example):\n",
        "    input_text = f\"Context: {example['context']} Question: {example['question']}\"\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        input_text,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            example[\"answer\"],\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = [\n",
        "        label if label != tokenizer.pad_token_id else -100\n",
        "        for label in labels[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_bart, batched=False, remove_columns=dataset.column_names)\n",
        "\n",
        "# Setting up LoRA model\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Setting training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bart-RAG\",\n",
        "    eval_strategy=\"no\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=10,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Training\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning (T5-Large)"
      ],
      "metadata": {
        "id": "cOHMU7dzXFbD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAMVj3DC26qI"
      },
      "outputs": [],
      "source": [
        "# Environmental setup - ensure sufficient memory\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Data Preparation\n",
        "\n",
        "def flatten_text(text):\n",
        "    return text.replace('\\n', ' ').replace('  ', ' ').strip()\n",
        "\n",
        "# Creating input/output format\n",
        "reformatted_dataset = [\n",
        "    {\n",
        "        \"input\": f\"question: {item['question']} context: {flatten_text(item['context'])}\",\n",
        "        \"output\": item[\"answer\"]\n",
        "    }\n",
        "    for item in expanded_data\n",
        "]\n",
        "\n",
        "# Converting list to dataset\n",
        "dataset = Dataset.from_list(reformatted_dataset)\n",
        "\n",
        "\n",
        "# Tokenizing\n",
        "model = \"t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "def tokenize(example):\n",
        "    # Tokenize input\n",
        "    model_inputs = tokenizer(\n",
        "        example[\"input\"],\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "    # Tokenize target (output)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            example[\"output\"],\n",
        "            max_length=128,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "    # Transforming padded token positions\n",
        "    model_inputs[\"labels\"] = [\n",
        "        l if l != tokenizer.pad_token_id else -100 for l in labels[\"input_ids\"]\n",
        "    ]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Setting up LoRA model\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Setting training arguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./t5-RAG\",\n",
        "    eval_strategy=\"no\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=10,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Training\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Saving Fine-tuned T5 large model parameters"
      ],
      "metadata": {
        "id": "PZjBeiGEXMTt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NRYR06-K7mb"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/t5-lora-final\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/t5-lora-final\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t_VEvjTLRUN"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/t5-lora-final\")\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")\n",
        "model = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/t5-lora-final\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOnHXb6MMuEYAxfP1XgXfIh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}